{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: basic usage\n",
    "In this tutorial, we apply DDN to two simple synthetic datasets, which is generated under two different network structures.\n",
    "After DDN detects the two networks, we visualize the common and differential networks, as well as getting the edges in each network.\n",
    "Since we use synthetic data in this example, we can compare the DDN results with groud truth. We can also study the impact of different hyper-parameters.\n",
    "\n",
    "Comparing with the previous demo, we go into slightly more details, and do not use the `pipeline` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import some modules. The `ddn` packages implements the DDN 3.0 algorithm. The `tools_export` modules provides functions for generating useful outputs.\n",
    "The `visualize` module provides some simple function to draw the networks based on the `networkx` package.\n",
    "\n",
    "The `tools`, `simulation`, and `performance` modules are used here to generate synthetic data and evaluate performance.\n",
    "They are not needed if you are working on a real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ddn3 import ddn, tools_export, visualize\n",
    "from ddn3 import tools, simulation, performance\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "DDN needs two datasets as input, as well as a list of gene names for output and visualization.\n",
    "Each dataset should be a NumPy array. Each row of the array represent a sample, and each column represent a feature (like genes).\n",
    "The two datasets may have different number of samples, but must have the same number of features.\n",
    "\n",
    "Here we generate two synthetic datasets, `dat1` and `dat2`. They are generated based on a common graph with 40 nodes.\n",
    "There are 20 edges in this graph, connecting node0 with node20, node1 with node21, etc.\n",
    "The two conditions are created by shuffling five of the edges in common network.\n",
    "Each condition has 100 samples. The names of genes are given in `gene_nemas`.\n",
    "\n",
    "We define the common network is the set of all edges that is present in both conditions, and the differential network as the set of edges that present in only one of the conditions.\n",
    "The ground truth common and differential network in this synthetic data is saved in `comm_gt` and `diff_gt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "n_node = 40\n",
    "n_sample1 = 100\n",
    "n_sample2 = 100\n",
    "n_shuf = 5\n",
    "g1_prec, g2_prec = simulation.create_pair_graph(n_node=n_node, corr=0.75, n_shuf=n_shuf)\n",
    "comm_gt, diff_gt = tools.get_common_diff_net_topo([g1_prec, g2_prec])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give some arbitrary names to these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "gene_names = [f\"Gene{i}\" for i in range(n_node)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is generated by sampling the covariance matrix of each condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "g1_cov, _ = simulation.create_cov_prec_mat(g1_prec)\n",
    "g2_cov, _ = simulation.create_cov_prec_mat(g2_prec)\n",
    "dat1 = tools.gen_mv(g1_cov, n_sample1)\n",
    "dat2 = tools.gen_mv(g2_cov, n_sample2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running DDN 3.0 and visualize the networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDN requires two hyper-parameters, $\\lambda_1$ and $\\lambda_2$. \n",
    "Here $\\lambda_1$ controls the overall sparsity of the two networks, and $\\lambda_2$ controls the similarity betwwen the two graphs.\n",
    "The typical value of $\\lambda_1$ is between 0.1 and 0.8, while $\\lambda_2$ is between 0.02 and 0.2.\n",
    "In this dataset, $\\lambda_1=0.3$ and $\\lambda_2=0.1$ turns out to be a good choice, as we have the ground truth network structure.\n",
    "\n",
    "In real applications, it is advised to utilize the prior knowledge to determine the desired density of the graphs.\n",
    "Specifically, one may run DDN with a range of $\\lambda_1$s and $\\lambda_2$s, and pick the network that is most usable.\n",
    "Alternatively, we provide various functionalities of hyperparameter setting, and we cover that topic in `demo_parameter_tuning.ipynb`.\n",
    "\n",
    "The first time you run DDN 3.0, it takes a short time for Numba to compile the code. If you run it again later, it will be much faster.\n",
    "\n",
    "The output of DDN is two coefficient matrices, one for each condition. For a graph with $N$ features, each matrix has dimension $N\\times N$.\n",
    "The value of element in row $i$ and column $j$ means the estimated coefficient between the $i$ th feature and the $j$ th feature.\n",
    "If the value is non-zero, it implies there could be dependencies between these two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "omega1, omega2 = ddn.ddn(dat1, dat2, lambda1=0.3, lambda2=0.1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "# np.savez(\n",
    "#     \"two_part_network.npz\",\n",
    "#     dat1=dat1,\n",
    "#     dat2=dat2,\n",
    "#     omega1=omega1,\n",
    "#     omega2=omega2,\n",
    "#     lambda1=0.3,\n",
    "#     lambda2=0.1,\n",
    "#     g1_prec=g1_prec,\n",
    "#     g2_prec=g2_prec,\n",
    "#     g1_cov=g1_cov,\n",
    "#     g2_cov=g2_cov,\n",
    "# )"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then extract common and differential networks from two estimated coefficient matrices.\n",
    "As DDN use regression to learn the dependency of each node with all other nodes, the coefficient matrix is not necessarily symmetric.\n",
    "For each matrix, we take the average with its transpose to make it symmetric. Then we apply a very small threshold to eliminate too small values.\n",
    "The resulting matrix is binarized as the adjacency matrix. Then we extract common and differential networks from these two adjacency matrices.\n",
    "\n",
    "All these tasks can be performed using the `get_diff_comm_net_for_plot` function in the `tools_export` module.\n",
    "The output `comm_edge` is a pandas data frame that reports edges in both networks. The also provides `weight` for each edge.\n",
    "The output `dif1_edge` provides edges that only present in condition 1 (`dat1`), and `dif2_edge` provides edges that only present in condition 2 (`dat2`).\n",
    "The `diff_edge` combines `dif1_edge` and `dif2_edge`, and is the differential network we refer to later.\n",
    "The `node_non_isolated` list the gene names that are not isolated in either common or differential networks. \n",
    "In other words, it includes nodes that has at least a neighbor. This list can be useful for visualization, as it eliminates many irrelevant nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "comm_edge, dif1_edge, dif2_edge, diff_edge, node_non_isolated = tools_export.get_diff_comm_net_for_plot(omega1, omega2, gene_names)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data frames `comm_edge`, `dif1_edge`, `dif2_edge` and `diff_edge` also provides the weigth of the edge. \n",
    "Here we use the absolute value of the coefficient as the weight.\n",
    "In `diff_edge`, to distinguish edges from two conditions, a `condition` column is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "diff_edge"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the common networks using the `draw_network_for_ddn`function in the `visualize` module.\n",
    "The function requires the `comm_edge` data frame, as well as the list of gene names.\n",
    "For common networks, we need to set `mode=\"common\"`.\n",
    "\n",
    "The size of the node will represent the degree of each node. The thickness and darkness of the edge will represent the edge weight.\n",
    "In this synthetic data, all nodes have same degree and all edges have the same weigth, so the node sizes and edge thickness are the same.\n",
    "\n",
    "To adjust figure size, font size, or node size, you may change the correspoding parameters in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "visualize.draw_network_for_ddn(\n",
    "    comm_edge,\n",
    "    node_non_isolated,\n",
    "    fig_size=10,\n",
    "    font_size_scale=1,\n",
    "    node_size_scale=1,\n",
    "    part_number=1,\n",
    "    mode=\"common\",\n",
    "    export_pdf=False,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can visualize the differential network. The blue edges means that the edge only occurs in condition 1. The blue edge means it only occurs in condition 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "visualize.draw_network_for_ddn(\n",
    "    diff_edge,\n",
    "    node_non_isolated,\n",
    "    fig_size=10,\n",
    "    font_size_scale=1,\n",
    "    node_size_scale=1,\n",
    "    part_number=1,\n",
    "    mode=\"diff\",\n",
    "    export_pdf=False,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the performance against ground truth\n",
    "We can quantitatively evaluate the performance of DDN on this synthetic data.\n",
    "As discussed above, we first need to modify the coefficient matrices to the common and differential network adjacency matrices.\n",
    "Then we can compare them with the groud truth.\n",
    "\n",
    "The function `get_error_measure_two_theta` gives five performance measures:\n",
    "true positive, false positive, true positive rate, false positive rate, precision.\n",
    "As can be seen, for this data, the results are perfect, with all edges correctly detected and no false positive is included.\n",
    "Note that due to randonness in the data, it is possible to observe some small errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "comm_est_ddn, diff_est_ddn = tools.get_common_diff_net_topo([omega1, omega2])\n",
    "eval_comm = performance.get_error_measure_two_theta(comm_est_ddn, comm_gt)\n",
    "eval_diff = performance.get_error_measure_two_theta(diff_est_ddn, diff_gt)\n",
    "print(\"Performance measures: true positive, false positive, true positive rate, false positive rate, precision\")\n",
    "print(\"Common network \", eval_comm)\n",
    "print(\"Differential network \", eval_diff)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the estimated and the ground truth adjacency matrix of common network (we removed the diagonal elements).\n",
    "By comparing the two figures, we can confirm the results are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "plt.imshow(comm_est_ddn)\n",
    "plt.show()\n",
    "plt.imshow(comm_gt)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the estimated and the ground truth adjacency matrix of differential network (we remove the diagonal elements).\n",
    "In this data, the results are also correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "plt.imshow(diff_est_ddn)\n",
    "plt.show()\n",
    "plt.imshow(diff_gt)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study the impact of $\\lambda_1$\n",
    "We can also study the performance of DDN under different $\\lambda_1$ values.\n",
    "We start from a small value of 0.02, and go all the way to 1.0, with a step size of 0.02.\n",
    "This can be achieved by writing a simple `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "lambda1_rg = np.arange(0.02, 1.01, 0.02)\n",
    "res_comm_ddn = np.zeros((len(lambda1_rg), 5))\n",
    "res_diff_ddn = np.zeros((len(lambda1_rg), 5))\n",
    "for i, lamb in enumerate(lambda1_rg):\n",
    "    out_ddn = ddn.ddn(dat1, dat2, lambda1=lamb, lambda2=0.1)\n",
    "    comm_est, diff_est = tools.get_common_diff_net_topo(out_ddn)\n",
    "    res_comm_ddn[i] = performance.get_error_measure_two_theta(comm_est, comm_gt)\n",
    "    res_diff_ddn[i] = performance.get_error_measure_two_theta(diff_est, diff_gt)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the number of ture positive edges in the common network as a function of $\\lambda_1$.\n",
    "It is clear that for smaller $\\lambda_1$ values, all common edges are detectd.\n",
    "With larger $\\lambda_1$, more and more edges are lost due to stronger penalization, and ultimately no edges are detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "plt.plot(res_comm_ddn[:,0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the number of ture positive edges in differential network as a function of $\\lambda_1$.\n",
    "The condition is somewhat different. When $\\lambda_1$ is too small, differential edges are not well detected, as many of them are just considered as common edges.\n",
    "Larger $\\lambda_1$ help to revealt these differential edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "plt.plot(res_diff_ddn[:,0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choie of $\\lambda_2$ also influence the performance. We do no show it here, but you are free to try.\n",
    "The effects will be more clear with a smaller sample size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
